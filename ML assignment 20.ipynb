{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69d822ae",
   "metadata": {},
   "source": [
    "## 1. What is the underlying concept of Support Vector Machines?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8393e3e7",
   "metadata": {},
   "source": [
    "The underlying concept of Support Vector Machines (SVM) is to find the optimal hyperplane that best separates different classes in a dataset. SVM is a supervised machine learning algorithm used for classification and regression tasks.\n",
    "\n",
    "In a binary classification problem, SVM aims to find the hyperplane that maximizes the margin between the two classes. The margin is the distance between the hyperplane and the closest data points of each class, called support vectors. The idea is to select a hyperplane that has the largest margin, as it is more likely to generalize well on new, unseen data.\n",
    "\n",
    "The SVM algorithm works by transforming the data into a higher-dimensional space (using a kernel function) to find a separating hyperplane. The most common kernel functions are linear, polynomial, radial basis function (RBF), and sigmoid. Once the hyperplane is found, new data points can be classified based on which side of the hyperplane they fall.\n",
    "\n",
    "SVM is well-known for its ability to handle high-dimensional data, its robustness against overfitting, and its effectiveness in cases where the data is not linearly separable. It is widely used in various applications, such as image recognition, text classification, and bioinformatics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39086045",
   "metadata": {},
   "source": [
    "## 2. What is the concept of a support vector?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10ee135",
   "metadata": {},
   "source": [
    "In the context of Support Vector Machines (SVM), a support vector is a data point from the training dataset that lies closest to the decision boundary or hyperplane that separates the different classes. These support vectors are critical in determining the optimal hyperplane for classification.\n",
    "\n",
    "The key idea in SVM is to find the hyperplane that maximizes the margin between the support vectors of different classes. The margin is defined as the perpendicular distance between the hyperplane and the closest support vectors from each class. By maximizing the margin, SVM aims to create a decision boundary that can generalize well to new, unseen data.\n",
    "\n",
    "Support vectors play a crucial role in defining the optimal hyperplane because they are the data points that influence the position and orientation of the decision boundary. In essence, they act as the \"support\" for the hyperplane and give SVM its name.\n",
    "\n",
    "During the training phase of SVM, the algorithm selects the support vectors, and all other data points that are not support vectors do not affect the position of the hyperplane. This property makes SVM computationally efficient, especially when dealing with large datasets.\n",
    "\n",
    "In summary, support vectors are the critical data points that determine the optimal decision boundary in SVM, and the algorithm focuses on finding these points to achieve a well-generalized model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1197114f",
   "metadata": {},
   "source": [
    "## 3. When using SVMs, why is it necessary to scale the inputs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8e974f",
   "metadata": {},
   "source": [
    "When using Support Vector Machines (SVMs), it is necessary to scale the inputs or features because SVM is sensitive to the scale of the input data. Scaling the features ensures that all the features contribute equally to the model's decision-making process and helps improve the performance and convergence of the SVM algorithm. There are a few reasons why feature scaling is important in SVM:\n",
    "\n",
    "1. Influence on the Distance Metric: SVM aims to find the optimal hyperplane that maximizes the margin between the support vectors of different classes. The distance between data points and the decision boundary is calculated using the dot product of the feature vectors. If the features have significantly different scales, features with larger scales will dominate the distance calculations, while smaller-scale features may have a negligible impact.\n",
    "\n",
    "2. Convergence and Training Speed: Scaling the features can improve the convergence and training speed of the SVM algorithm. Since SVM involves solving a convex optimization problem, well-scaled features lead to a smoother and more well-behaved optimization landscape, making it easier and faster for the algorithm to find the optimal solution.\n",
    "\n",
    "3. Regularization: SVM includes a regularization term (C parameter) that controls the trade-off between maximizing the margin and minimizing the classification errors. The regularization term is sensitive to the scale of the features, and unbalanced scales can impact the regularization process.\n",
    "\n",
    "Common scaling methods include standardization (scaling to have zero mean and unit variance) or normalization (scaling to a fixed range, such as [0, 1] or [-1, 1]). It is essential to apply the same scaling to the training and test datasets to ensure consistency in the feature space during prediction.\n",
    "\n",
    "In summary, scaling the inputs in SVM ensures that all features have an equal influence on the model's decision boundary, leading to better convergence, improved performance, and more reliable model predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6e472f",
   "metadata": {},
   "source": [
    "## 4. When an SVM classifier classifies a case, can it output a confidence score? What about a percentage chance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf3fc07",
   "metadata": {},
   "source": [
    "Yes, an SVM classifier can output a confidence score or probability estimates, but it depends on the specific variant of SVM being used and the settings chosen during training and prediction.\n",
    "\n",
    "1. Standard SVM: In the traditional SVM, the classifier directly assigns class labels to the input data points based on which side of the decision boundary they lie. There are no inherent confidence scores or probability estimates associated with the predictions.\n",
    "\n",
    "2. Probabilistic SVM: Some SVM implementations, such as the `SVC` class in Scikit-learn with `probability=True`, allow for probability estimates using Platt scaling or other methods. Platt scaling is a technique that converts the decision scores generated by the SVM into probabilities. These probabilities represent the likelihood that a data point belongs to a specific class. However, these probabilities are not percentage chances in the sense of giving the exact likelihood, and they should be interpreted with caution, especially if the model was not explicitly trained for probabilistic outputs.\n",
    "\n",
    "Keep in mind that SVMs are primarily designed for binary classification tasks. For multiclass classification problems, SVM classifiers can be extended using one-vs-one or one-vs-all strategies. In such cases, the confidence scores or probability estimates may be computed for each individual binary classifier, and the final multiclass prediction may involve combining these scores in some way.\n",
    "\n",
    "If you need well-calibrated probabilities or percentage chances for your classification problem, it might be better to consider other classifiers like logistic regression or gradient boosting, which are more directly suited for providing probability estimates. Additionally, deep learning models, such as neural networks, are commonly used for tasks where probability estimation is essential."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04becf2b",
   "metadata": {},
   "source": [
    "## 5. Should you train a model on a training set with millions of instances and hundreds of features using the primal or dual form of the SVM problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d690ed",
   "metadata": {},
   "source": [
    "When dealing with a training set with millions of instances and hundreds of features, it is generally more efficient to use the dual form of the SVM problem. The dual form of the SVM algorithm is computationally more efficient when the number of training instances is much larger than the number of features.\n",
    "\n",
    "The primal form of the SVM problem involves optimizing the model's weights and bias directly in the original feature space. In this case, the computational complexity grows linearly with the number of instances, making it less suitable for large datasets.\n",
    "\n",
    "On the other hand, the dual form of the SVM problem involves optimizing the model using the inner products between instances. This allows the algorithm to work with the so-called \"kernel trick,\" where the inner products are replaced by kernel functions. The dual form's computational complexity depends on the number of instances and the number of support vectors, which are typically much smaller than the total number of instances.\n",
    "\n",
    "In summary, for large-scale datasets with millions of instances and hundreds of features, the dual form of the SVM problem is more efficient and often preferred over the primal form. The dual form allows for faster computation, especially when using kernel functions to capture non-linear relationships between data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc95056d",
   "metadata": {},
   "source": [
    "## 6. Let&#39;s say you&#39;ve used an RBF kernel to train an SVM classifier, but it appears to underfit the training collection. Is it better to raise or lower (gamma)? What about the letter C?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1988a7bd",
   "metadata": {},
   "source": [
    "If an SVM classifier with an RBF kernel is underfitting the training data, you can adjust the hyperparameters gamma and C to try to improve the model's performance.\n",
    "\n",
    "1. Gamma (γ): The gamma parameter determines the influence of each individual training sample on the decision boundary. Higher gamma values make the model more sensitive to individual data points, which can lead to overfitting. Conversely, lower gamma values make the decision boundary smoother and can help reduce overfitting.\n",
    "\n",
    "- To address underfitting, you should lower the value of gamma. This will make the RBF kernel less sensitive to individual data points and allow the decision boundary to capture broader patterns in the data.\n",
    "\n",
    "2. C: The C parameter is the regularization parameter in SVM. It controls the trade-off between maximizing the margin and minimizing the classification error on the training data. Larger C values penalize misclassification more severely, leading to more complex models and potentially overfitting. Smaller C values introduce more regularization and can help reduce overfitting.\n",
    "\n",
    "- To address underfitting, you should increase the value of C. This will allow the SVM to fit the training data more closely and potentially capture more complex patterns in the data.\n",
    "\n",
    "It's important to note that adjusting hyperparameters like gamma and C should be done in a principled manner, using techniques like cross-validation, to avoid overfitting on the validation data as well. Hyperparameter tuning is an iterative process where you experiment with different values and evaluate the model's performance on a separate validation set to find the best combination of hyperparameters that generalizes well to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251b014b",
   "metadata": {},
   "source": [
    "## 7. To solve the soft margin linear SVM classifier problem with an off-the-shelf QP solver, how should the QP parameters (H, f, A, and b) be set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4445afbc",
   "metadata": {},
   "source": [
    "To solve the soft margin linear SVM classifier problem using an off-the-shelf quadratic programming (QP) solver, you need to set the QP parameters (H, f, A, and b) as follows:\n",
    "\n",
    "1. Objective Function (H, f):\n",
    "The objective function of the soft margin SVM can be represented as a quadratic programming problem, where you aim to minimize the hinge loss plus the regularization term. The objective function can be written as:\n",
    "\n",
    "minimize: (1/2) * w^T * w + C * Σ(max(0, 1 - y_i * (w^T * x_i + b))^2)\n",
    "\n",
    "Where:\n",
    "- w: Weight vector\n",
    "- C: The regularization parameter (a hyperparameter)\n",
    "- y_i: The target class label of the i-th instance\n",
    "- x_i: The feature vector of the i-th instance\n",
    "- b: The bias term\n",
    "\n",
    "H and f are the matrices required for the quadratic form of the objective function, and they can be computed based on the data and C value.\n",
    "\n",
    "2. Constraints (A, b):\n",
    "The soft margin SVM has the following constraints:\n",
    "\n",
    "- y_i * (w^T * x_i + b) >= 1 - ξ_i, for all training instances\n",
    "\n",
    "Where:\n",
    "- ξ_i: The slack variable representing the margin violation for the i-th instance.\n",
    "\n",
    "The constraints can be written in the form A * α >= b, where A and b are matrices that encode the constraints based on the training data.\n",
    "\n",
    "To summarize, the QP parameters are set as follows:\n",
    "\n",
    "H: Matrix derived from the regularization term and data.\n",
    "f: Vector derived from the regularization term and data.\n",
    "A: Matrix derived from the constraints of the soft margin SVM.\n",
    "b: Vector derived from the constraints of the soft margin SVM.\n",
    "\n",
    "Once you have set these parameters, you can use an off-the-shelf QP solver to find the optimal values of w and b that minimize the objective function and satisfy the constraints, thus obtaining the optimal hyperplane for the soft margin SVM classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9205eb92",
   "metadata": {},
   "source": [
    "## 8. On a linearly separable dataset, train a LinearSVC. Then, using the same dataset, train an SVC and an SGDClassifier. See if you can get them to make a model that is similar to yours."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce84630",
   "metadata": {},
   "source": [
    "Training different linear classifiers on a linearly separable dataset and comparing their performance can be an interesting exercise. Here's how you can do it using scikit-learn:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Create a linearly separable dataset\n",
    "X, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0,\n",
    "                           n_clusters_per_class=1, class_sep=2, random_state=42)\n",
    "\n",
    "# Train a LinearSVC classifier\n",
    "linear_svc = LinearSVC()\n",
    "linear_svc.fit(X, y)\n",
    "\n",
    "# Train an SVC classifier with a linear kernel\n",
    "svc_linear = SVC(kernel='linear')\n",
    "svc_linear.fit(X, y)\n",
    "\n",
    "# Train an SGDClassifier with 'hinge' loss (equivalent to linear SVM)\n",
    "sgd_classifier = SGDClassifier(loss='hinge')\n",
    "sgd_classifier.fit(X, y)\n",
    "\n",
    "# Plot the decision boundaries\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='k', s=100)\n",
    "plt.title('Linearly Separable Dataset')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "\n",
    "# Plot decision boundaries of the classifiers\n",
    "h = 0.02\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "for clf, title in [(linear_svc, 'LinearSVC'),\n",
    "                   (svc_linear, 'SVC (Linear Kernel)'),\n",
    "                   (sgd_classifier, 'SGDClassifier (Linear SVM)')]:\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')\n",
    "    plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=100,\n",
    "                facecolors='none', edgecolors='k', label=title + ' Support Vectors')\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "This code will plot the linearly separable dataset and decision boundaries of the three classifiers. The LinearSVC and SVC with a linear kernel should produce similar decision boundaries, as they are both linear SVM classifiers. The SGDClassifier with 'hinge' loss will also be similar, as it is another form of linear SVM. The decision boundaries should effectively separate the two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09471a3e",
   "metadata": {},
   "source": [
    "## 9. On the MNIST dataset, train an SVM classifier. You&#39;ll need to use one-versus-the-rest to assign all 10 digits because SVM classifiers are binary classifiers. To accelerate up the process, you might want to tune the hyperparameters using small validation sets. What level of precision can you achieve?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53626bf7",
   "metadata": {},
   "source": [
    "Training an SVM classifier on the MNIST dataset can take some time, especially if you want to tune the hyperparameters. Below is an example code using scikit-learn to train an SVM classifier with a small validation set for hyperparameter tuning:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist = fetch_openml('mnist_784')\n",
    "X, y = mnist['data'], mnist['target']\n",
    "\n",
    "# Convert the labels from strings to integers\n",
    "y = y.astype(int)\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42)\n",
    "\n",
    "# Hyperparameter tuning using a small validation set\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "}\n",
    "\n",
    "svm = SVC()\n",
    "grid_search = GridSearchCV(svm, param_grid, cv=2, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_val, y_val)\n",
    "\n",
    "# Get the best hyperparameters from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Train the SVM classifier with the best hyperparameters on the full training set\n",
    "svm = SVC(**best_params)\n",
    "svm.fit(X_train_full, y_train_full)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Calculate accuracy and other metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_rep)\n",
    "```\n",
    "\n",
    "Note that training an SVM classifier on the full MNIST dataset with a grid search for hyperparameter tuning can take a significant amount of time, depending on your machine's computational resources. You can achieve a reasonably high level of precision with the SVM classifier on MNIST, typically above 95% accuracy. However, the exact performance will depend on the hyperparameters and the size of the training set used for tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89e7e7e",
   "metadata": {},
   "source": [
    "## 10. On the California housing dataset, train an SVM regressor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746320d3",
   "metadata": {},
   "source": [
    "To train an SVM regressor on the California housing dataset, we first need to load the dataset and preprocess it. The California housing dataset is available in scikit-learn, so we can use it directly. Here's an example code to train an SVM regressor on the dataset:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the California housing dataset\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train the SVM regressor\n",
    "svm_regressor = SVR(kernel='linear')\n",
    "svm_regressor.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_regressor.predict(X_test_scaled)\n",
    "\n",
    "# Calculate mean squared error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "```\n",
    "\n",
    "In this code, we first load the California housing dataset using `fetch_california_housing` from scikit-learn. Then, we split the data into training and test sets using `train_test_split`. Next, we standardize the features using `StandardScaler` to scale them to have zero mean and unit variance. This step is essential for many machine learning algorithms, including SVM.\n",
    "\n",
    "We then create an SVM regressor with a linear kernel using `SVR` from scikit-learn and train it on the standardized training data. Finally, we make predictions on the test set and calculate the mean squared error to evaluate the performance of the SVM regressor.\n",
    "\n",
    "Note that the performance of the SVM regressor may vary depending on the hyperparameters, and you may want to perform hyperparameter tuning to achieve the best results for your specific task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
